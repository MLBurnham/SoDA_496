{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docketId</th>\n",
       "      <th>caseId</th>\n",
       "      <th>docket</th>\n",
       "      <th>justice</th>\n",
       "      <th>petitioner_vote</th>\n",
       "      <th>petitioner_pitch</th>\n",
       "      <th>respondent_pitch</th>\n",
       "      <th>lcDispositionDirection</th>\n",
       "      <th>justiceName</th>\n",
       "      <th>lagged_ideology</th>\n",
       "      <th>...</th>\n",
       "      <th>respondent_harvard_pos</th>\n",
       "      <th>petitioner_harvard_neg</th>\n",
       "      <th>respondent_harvard_neg</th>\n",
       "      <th>petitioner_liwc_pos</th>\n",
       "      <th>respondent_liwc_pos</th>\n",
       "      <th>petitioner_liwc_neg</th>\n",
       "      <th>respondent_liwc_neg</th>\n",
       "      <th>term</th>\n",
       "      <th>pitch_diff</th>\n",
       "      <th>conservative_lc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1982-038-01</td>\n",
       "      <td>1982-038</td>\n",
       "      <td>81-1802</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>0.098386</td>\n",
       "      <td>0.547079</td>\n",
       "      <td>2</td>\n",
       "      <td>SDOConnor</td>\n",
       "      <td>1.491</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1982</td>\n",
       "      <td>-0.448692</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1982-038-01</td>\n",
       "      <td>1982-038</td>\n",
       "      <td>81-1802</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.548233</td>\n",
       "      <td>-0.392148</td>\n",
       "      <td>2</td>\n",
       "      <td>WHRehnquist</td>\n",
       "      <td>4.036</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1982</td>\n",
       "      <td>-0.156086</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1982-038-01</td>\n",
       "      <td>1982-038</td>\n",
       "      <td>81-1802</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.329200</td>\n",
       "      <td>-0.881118</td>\n",
       "      <td>2</td>\n",
       "      <td>WEBurger</td>\n",
       "      <td>1.491</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1982</td>\n",
       "      <td>-0.448082</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1982-038-01</td>\n",
       "      <td>1982-038</td>\n",
       "      <td>81-1802</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.327295</td>\n",
       "      <td>0.031076</td>\n",
       "      <td>2</td>\n",
       "      <td>JPStevens</td>\n",
       "      <td>-0.265</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1982</td>\n",
       "      <td>-0.358370</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1982-038-01</td>\n",
       "      <td>1982-038</td>\n",
       "      <td>81-1802</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008952</td>\n",
       "      <td>-0.011692</td>\n",
       "      <td>2</td>\n",
       "      <td>TMarshall</td>\n",
       "      <td>-3.684</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1982</td>\n",
       "      <td>0.020644</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      docketId    caseId   docket  justice  petitioner_vote  petitioner_pitch  \\\n",
       "0  1982-038-01  1982-038  81-1802      104                1          0.098386   \n",
       "1  1982-038-01  1982-038  81-1802      102                1         -0.548233   \n",
       "2  1982-038-01  1982-038  81-1802       99                1         -1.329200   \n",
       "3  1982-038-01  1982-038  81-1802      103                1         -0.327295   \n",
       "4  1982-038-01  1982-038  81-1802       98                1          0.008952   \n",
       "\n",
       "   respondent_pitch  lcDispositionDirection  justiceName  lagged_ideology  \\\n",
       "0          0.547079                       2    SDOConnor            1.491   \n",
       "1         -0.392148                       2  WHRehnquist            4.036   \n",
       "2         -0.881118                       2     WEBurger            1.491   \n",
       "3          0.031076                       2    JPStevens           -0.265   \n",
       "4         -0.011692                       2    TMarshall           -3.684   \n",
       "\n",
       "   ...  respondent_harvard_pos  petitioner_harvard_neg  \\\n",
       "0  ...                     1.0                     0.0   \n",
       "1  ...                     7.0                     3.0   \n",
       "2  ...                    18.0                     1.0   \n",
       "3  ...                     7.0                     1.0   \n",
       "4  ...                     4.0                     1.0   \n",
       "\n",
       "   respondent_harvard_neg  petitioner_liwc_pos  respondent_liwc_pos  \\\n",
       "0                     1.0                  3.0                  0.0   \n",
       "1                     3.0                  2.0                  2.0   \n",
       "2                     8.0                  1.0                 11.0   \n",
       "3                     2.0                  2.0                  2.0   \n",
       "4                     2.0                  0.0                  3.0   \n",
       "\n",
       "   petitioner_liwc_neg  respondent_liwc_neg  term  pitch_diff  conservative_lc  \n",
       "0                  1.0                  0.0  1982   -0.448692              0.0  \n",
       "1                  0.0                  1.0  1982   -0.156086              0.0  \n",
       "2                  0.0                  2.0  1982   -0.448082              0.0  \n",
       "3                  2.0                  1.0  1982   -0.358370              0.0  \n",
       "4                  1.0                  0.0  1982    0.020644              0.0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "just = pd.read_csv('justice_results.csv')\n",
    "just.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAL\n",
    "just['dal_petpos'] = just['petitioner_dal_pos']/just['petitioner_wc'] - just['respondent_dal_pos']/just['respondent_wc']\n",
    "just['dal_petneg'] = just['petitioner_dal_neg']/just['petitioner_wc'] - just['respondent_dal_neg']/just['respondent_wc']\n",
    "# Harvard\n",
    "just['harvard_petpos'] = just['petitioner_harvard_pos']/just['petitioner_wc'] - just['respondent_harvard_pos']/just['respondent_wc']\n",
    "just['harvard_petneg'] = just['petitioner_harvard_neg']/just['petitioner_wc'] - just['respondent_harvard_neg']/just['respondent_wc']\n",
    "# LIWC\n",
    "just['liwc_petpos'] = just['petitioner_liwc_pos']/just['petitioner_wc'] - just['respondent_liwc_pos']/just['respondent_wc']\n",
    "just['liwc_petneg'] = just['petitioner_liwc_neg']/just['petitioner_wc'] - just['respondent_liwc_neg']/just['respondent_wc']\n",
    "# Questions\n",
    "just['pet_questions'] = just['petitioner_count'] - just['respondent_count']\n",
    "# ideology x lower court conservative\n",
    "just['idealxLC'] = just['lagged_ideology'] * just['conservative_lc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to necessary columns\n",
    "just = just[['petitioner_vote',\n",
    "      'pitch_diff', \n",
    "      'dal_petpos',\n",
    "      'dal_petneg',\n",
    "      'harvard_petpos',\n",
    "      'harvard_petneg',\n",
    "      'liwc_petpos',\n",
    "      'liwc_petneg',\n",
    "      'pet_questions',\n",
    "      'conservative_lc',\n",
    "      'lagged_ideology',\n",
    "      'idealxLC',\n",
    "      'sgpetac',\n",
    "      'sgrespac',\n",
    "      'petac',\n",
    "      'respac',\n",
    "      'petNumStat',\n",
    "      'respNumStat',\n",
    "      'justiceName'      \n",
    "     ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy variables for justices\n",
    "just = pd.get_dummies(just, columns = ['justiceName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "just.to_csv('justice_results_clean.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice there is missing data that gets dropped between different iterations of the models. This inconsistency could result in a distributional shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "just = just.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5334672021419009\n",
      "0.6204819277108434\n"
     ]
    }
   ],
   "source": [
    "y = just['petitioner_vote']\n",
    "X = just.iloc[:,1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 1)\n",
    "\n",
    "mod1 = LogisticRegression(random_state = 1, solver = 'liblinear').fit(X_train, y_train)\n",
    "\n",
    "# majority class classifier\n",
    "print(y_test.mean())\n",
    "print(mod1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5334672021419009\n",
      "0.6204819277108434\n"
     ]
    }
   ],
   "source": [
    "dal = just.drop(labels = ['harvard_petpos', 'harvard_petneg', 'liwc_petpos', 'liwc_petneg'], axis = 1)\n",
    "\n",
    "y = dal['petitioner_vote']\n",
    "X = dal.iloc[:,1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 1)\n",
    "\n",
    "mod2= LogisticRegression(random_state = 1, solver = 'liblinear').fit(X_train, y_train)\n",
    "print(y_test.mean())\n",
    "print(mod2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harvard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5334672021419009\n",
      "0.6211512717536813\n"
     ]
    }
   ],
   "source": [
    "harvard = just.drop(labels = ['dal_petpos', 'dal_petneg', 'liwc_petpos', 'liwc_petneg'], axis = 1)\n",
    "\n",
    "y = harvard['petitioner_vote']\n",
    "X = harvard.iloc[:,1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 1)\n",
    "\n",
    "mod3= LogisticRegression(random_state = 1, solver = 'liblinear').fit(X_train, y_train)\n",
    "print(y_test.mean())\n",
    "print(mod3.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5334672021419009\n",
      "0.6231593038821954\n"
     ]
    }
   ],
   "source": [
    "liwc = just.drop(labels = ['dal_petpos', 'dal_petneg', 'harvard_petpos', 'harvard_petneg'], axis = 1)\n",
    "\n",
    "y = liwc['petitioner_vote']\n",
    "X = liwc.iloc[:,1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 1)\n",
    "\n",
    "mod4= LogisticRegression(random_state = 1, solver = 'liblinear').fit(X_train, y_train)\n",
    "print(y_test.mean())\n",
    "print(mod4.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pitch_diff': -0.21277442198119592,\n",
       " 'liwc_petpos': -0.5839041403081796,\n",
       " 'liwc_petneg': -0.45279296676773606,\n",
       " 'pet_questions': -0.058790223315654,\n",
       " 'conservative_lc': -0.01835391760323113,\n",
       " 'lagged_ideology': -0.08160147456831034,\n",
       " 'idealxLC': -0.23582846968665158,\n",
       " 'sgpetac': 0.4745546471680834,\n",
       " 'sgrespac': -0.6383512554322763,\n",
       " 'petac': 0.04556998001917423,\n",
       " 'respac': -0.06742169937467193,\n",
       " 'petNumStat': 0.03502738580940508,\n",
       " 'respNumStat': -0.0153239575993714,\n",
       " 'justiceName_AMKennedy': 0.4341542045871641,\n",
       " 'justiceName_AScalia': 0.5761031674948129,\n",
       " 'justiceName_BRWhite': 0.1131319814640918,\n",
       " 'justiceName_CThomas': -0.5046518860767433,\n",
       " 'justiceName_DHSouter': -0.12110367724243022,\n",
       " 'justiceName_EKagan': -0.19088138920310935,\n",
       " 'justiceName_HABlackmun': -0.5746266028243279,\n",
       " 'justiceName_JGRoberts': 0.529903313225748,\n",
       " 'justiceName_JPStevens': -0.7363636468387532,\n",
       " 'justiceName_LFPowell': 0.4835493519992183,\n",
       " 'justiceName_RBGinsburg': -0.4086669549493009,\n",
       " 'justiceName_SAAlito': 0.6388936620662329,\n",
       " 'justiceName_SDOConnor': 0.6561752949849209,\n",
       " 'justiceName_SGBreyer': -0.3037748448057064,\n",
       " 'justiceName_SSotomayor': -0.37453014122454203,\n",
       " 'justiceName_TMarshall': -0.9144950813829242,\n",
       " 'justiceName_WEBurger': 0.17534058469100053,\n",
       " 'justiceName_WHRehnquist': 0.5698862354152968}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get model coefficients\n",
    "dict(zip(mod4.feature_names_in_, mod4.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance W/ Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run a regression, what is the null model? i.e. what is the null hypothesis?\n",
    "What are the stars telling you? What is the coefficient \"significantly\" different from?\n",
    "\n",
    "The null model is that nothing has any effect on the DGP. That's spectacularly unlikely.\n",
    "\n",
    "What's the null model they assume in the supreme court paper? Fixed effects and fixed effects + pitch.\n",
    "\n",
    "Is this a good null model if you're trying to show that vocal pitch is predictive? Why or why not?\n",
    "\n",
    "What might be a more appropriate null model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5334672021419009\n",
      "0.536813922356091\n"
     ]
    }
   ],
   "source": [
    "# Fixed effects only\n",
    "y = liwc['petitioner_vote']\n",
    "X = liwc.iloc[:,14:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 1)\n",
    "\n",
    "mod4= LogisticRegression(random_state = 1, solver = 'liblinear').fit(X_train, y_train)\n",
    "print(y_test.mean())\n",
    "print(mod4.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5334672021419009\n",
      "0.5522088353413654\n"
     ]
    }
   ],
   "source": [
    "# Fixed effects and pitch\n",
    "y = liwc['petitioner_vote']\n",
    "X = liwc.iloc[:,np.r_[1,14:32]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 1)\n",
    "\n",
    "mod4= LogisticRegression(random_state = 1, solver = 'liblinear').fit(X_train, y_train)\n",
    "print(y_test.mean())\n",
    "print(mod4.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5334672021419009\n",
      "0.6251673360107095\n"
     ]
    }
   ],
   "source": [
    "# LIWC Model without pitch difference\n",
    "y = liwc['petitioner_vote']\n",
    "X = liwc.iloc[:,2:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 1)\n",
    "\n",
    "mod4= LogisticRegression(random_state = 1, solver = 'liblinear').fit(X_train, y_train)\n",
    "print(y_test.mean())\n",
    "print(mod4.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5334672021419009\n",
      "0.6238286479250335\n"
     ]
    }
   ],
   "source": [
    "# LIWC Model without pitch and positive/negative words\n",
    "y = liwc['petitioner_vote']\n",
    "X = liwc.iloc[:,4:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 1)\n",
    "\n",
    "mod4= LogisticRegression(random_state = 1, solver = 'liblinear').fit(X_train, y_train)\n",
    "print(y_test.mean())\n",
    "print(mod4.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance with Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference between effect size and feature importance? Is a regression coefficient feature importance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6653279785809906"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full model\n",
    "y = liwc['petitioner_vote']\n",
    "X = liwc.iloc[:,1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 1)\n",
    "\n",
    "liwcforest = RandomForestClassifier(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "liwcforest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6693440428380187"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without pitch\n",
    "y = liwc['petitioner_vote']\n",
    "X = liwc.iloc[:,2:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 1)\n",
    "\n",
    "liwcforest = RandomForestClassifier(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "liwcforest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6847389558232931"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without emotion\n",
    "y = liwc['petitioner_vote']\n",
    "X = liwc.iloc[:,4:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 1)\n",
    "\n",
    "liwcforest = RandomForestClassifier(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "liwcforest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5488621151271754"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixed effect only\n",
    "y = liwc['petitioner_vote']\n",
    "X = liwc.iloc[:,12:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 1)\n",
    "\n",
    "liwcforest = RandomForestClassifier(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "liwcforest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1.56 s\n",
      "Wall time: 4.36 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pitch_diff': 0.016465863453815198,\n",
       " 'liwc_petpos': 0.004551539491298484,\n",
       " 'liwc_petneg': 0.004484605087014681,\n",
       " 'pet_questions': 0.02463186077643902,\n",
       " 'conservative_lc': -0.0006024096385542687,\n",
       " 'lagged_ideology': 0.005622489959839305,\n",
       " 'idealxLC': 0.007496653279785759,\n",
       " 'sgpetac': 0.02489959839357423,\n",
       " 'sgrespac': 0.025100401606425647,\n",
       " 'petac': 0.031057563587684012,\n",
       " 'respac': 0.04089692101740291,\n",
       " 'petNumStat': 0.019611780455153882,\n",
       " 'respNumStat': 0.029585006693440375,\n",
       " 'justiceName_AMKennedy': -0.001874163319946487,\n",
       " 'justiceName_AScalia': -0.00046854082998665225,\n",
       " 'justiceName_BRWhite': -6.69344042838027e-05,\n",
       " 'justiceName_CThomas': 0.0,\n",
       " 'justiceName_DHSouter': -0.001004016064257074,\n",
       " 'justiceName_EKagan': -0.0006024096385542355,\n",
       " 'justiceName_HABlackmun': 0.0,\n",
       " 'justiceName_JGRoberts': -0.004551539491298584,\n",
       " 'justiceName_JPStevens': 0.0013386880856759876,\n",
       " 'justiceName_LFPowell': 0.0,\n",
       " 'justiceName_RBGinsburg': -0.0028112449799197136,\n",
       " 'justiceName_SAAlito': 0.0008701472556893686,\n",
       " 'justiceName_SDOConnor': 0.0025435073627844028,\n",
       " 'justiceName_SGBreyer': 0.0030789825970548245,\n",
       " 'justiceName_SSotomayor': -6.693440428383602e-05,\n",
       " 'justiceName_TMarshall': -1.1102230246251566e-17,\n",
       " 'justiceName_WEBurger': 0.0007362784471217187,\n",
       " 'justiceName_WHRehnquist': -0.0016733601070950787}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "liwcimport = permutation_importance(\n",
    "    liwcforest, X_test, y_test, n_repeats=10, random_state=1, n_jobs=2\n",
    ")\n",
    "\n",
    "dict(zip(liwcforest.feature_names_in_, result.importances_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over and under fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What causes overfitting?\n",
    "- lack of training data not representing the entire distribution of data\n",
    "- over-specified model is exploiting artifacts in the training data\n",
    "\n",
    "Solutions:\n",
    "- Increase the training data\n",
    "- cross validation (kind of)\n",
    "- use a more parsimonious model\n",
    "- regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6827309236947792"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full model\n",
    "y = liwc['petitioner_vote']\n",
    "X = liwc.iloc[:,1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .1, random_state = 1) # manipulate the size of the training data set\n",
    "\n",
    "liwcforest = RandomForestClassifier(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "liwcforest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5334672021419009\n",
      "0.6251673360107095\n"
     ]
    }
   ],
   "source": [
    "liwc = just.drop(labels = ['dal_petpos', 'dal_petneg', 'harvard_petpos', 'harvard_petneg'], axis = 1)\n",
    "\n",
    "y = liwc['petitioner_vote']\n",
    "X = liwc.iloc[:,1:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 1)\n",
    "\n",
    "mod4= LogisticRegressionCV(cv = 5, random_state = 1, solver = 'liblinear').fit(X_train, y_train)\n",
    "print(y_test.mean())\n",
    "print(mod4.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
